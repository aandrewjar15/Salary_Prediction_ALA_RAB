# -*- coding: utf-8 -*-
"""Salary_prediction_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yV_Yxl6dd9Y4x5BDAGSAky-_sX_Xzk5P
"""

# === Code Cell 1: load libraries & define path ===
import pandas as pd
!pip install ipywidgets
!pip install streamlit


raw_path = '/content/drive/MyDrive/Colab Notebooks/kaggle_survey_2022_responses.csv'

# === Code Cell 2: read CSV, drop first column & second-row metadata ===
# - header=0 uses the “Q#” codes in row 0 as column names
# - skiprows=[1] drops the 2nd line (the full question text)
df = pd.read_csv(raw_path, header=0, skiprows=[1])

# drop the first column (Duration)
df = df.drop(df.columns[0], axis=1)

df.head()

# === Code Cell 3: recode all Q#_# columns to 0/1 ===
import re

# identify every column of the form Q<number>_<number>
dummy_cols = [col for col in df.columns if re.fullmatch(r'Q\d+_\d+', col)]

# convert non-null → 1, null → 0
df[dummy_cols] = df[dummy_cols].notna().astype(int)

df.head()

# === Code Cell 4: drop columns with ≥70% missing values (except for Q29) ===

# Calculate fraction of missing values per column
missing_frac = df.isna().mean()

# Identify columns to drop: those with >=70% missing, excluding Q29
to_drop = [col for col, frac in missing_frac.items() if frac >= 0.7 and col != 'Q29']

# Drop the identified columns from df
df = df.drop(columns=to_drop)

# List the dropped columns
print("Dropped columns (>=70% missing, excluding Q29):")
for col in to_drop:
    print(f" - {col}")

# === Code Cell 5: show all unique values for every Question in the DataFrame ===
for col in df.columns:
    if col.startswith('Q'):
        print(f"--- {col} ---")
        # dropna() to exclude missing entries
        uniques = sorted(df[col].dropna().unique().tolist())
        print(uniques)
        print()

# === Code Cell 6: distribution & summary statistics for Q29 (salary range) ===
import pandas as pd

# 1. Distribution: counts and percentages
dist_counts = df['Q29'].value_counts(dropna=False)
dist_pct = df['Q29'].value_counts(normalize=True, dropna=False) * 100

print("Q29 Salary Range Distribution (counts):")
print(dist_counts)
print("\nQ29 Salary Range Distribution (percentages):")
print(dist_pct.round(2))

# 2. Summary statistics for the categorical variable
print("\nSummary statistics for Q29:")
print(df['Q29'].describe())

"""**1) What are the most significant factors driving Data Scientist salaries?**"""

# === Code Cell 7: train/test split & random forest to identify Q’s most predictive of Q29 (salary) ===
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# 1. Filter to rows with non-missing salary
df_model = df[df['Q29'].notna()].copy()

# 2. Encode the salary ranges as numeric labels
le_salary = LabelEncoder()
df_model['salary_label'] = le_salary.fit_transform(df_model['Q29'])

# 3. Prepare feature matrix X and target y
y = df_model['salary_label']
X = df_model.drop(columns=['Q29', 'salary_label'])

# 4. One-hot encode any remaining object columns (e.g., Q2, Q3, Q4, Q5, Q8, Q11, Q16)
X_encoded = pd.get_dummies(X, drop_first=True)

# 5. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42, stratify=y
)

# 6. Fit a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

# 7. Extract and sort feature importances
importances = pd.Series(rf.feature_importances_, index=X_encoded.columns)
importances_sorted = importances.sort_values(ascending=False)

# 8. Display top 20 most important Q-variables
print("Top 20 features most predictive of salary (Q29):")
print(importances_sorted.head(20))

"""# === **What are the most significant factors driving Data Scientist salaries?** ===
**Interpretation of Top 10 Features Most Predictive of Salary (Q29)**

1. **Country of Residence – United States of America**  
   Q4 asks “In which country do you currently reside?” and the United States emerges as the single strongest predictor of higher salary, reflecting both market demand and cost-of-living differentials

2. **Education Level – Master’s degree**  
   Q8 asks about highest formal education attained. Holding a Master’s degree is highly predictive of higher compensation, underscoring the value employers place on advanced credentials

3. **Helpful Resource – Video platforms (YouTube, Twitch, etc)**  
   Q7 asks “What products or platforms did you find most helpful when you first started studying data science?”  Respondents who cited video platforms early on tend to command higher salaries, suggesting that scalable, visual tutorials may accelerate skill acquisition

4. **Preferred Media – YouTube**  
   Q44 asks “Who/what are your favorite media sources that report on data science topics?”  Those who follow YouTube channels for their industry news show higher earnings, perhaps because they stay on the leading edge of practical techniques

5. **Country of Residence – India**  
   The second country signal (Q4 = India) also ranks highly, reflecting that regional nuances (e.g. local salary bands, outsourcing hubs) matter strongly when modeling compensation
6. **Preferred Media – Kaggle**  
   Q44_4 = Kaggle (notebooks, competitions) also predicts salary: active engagement on Kaggle’s platform correlates with better pay

7. **Course Platform – Coursera**  
   Q6_1 asks “On which platforms have you begun or completed data science courses?”  Completing courses on Coursera is a positive signal—likely because it denotes structured, peer-reviewed learning
8. **Helpful Resource – Online courses (Coursera, edX, etc)**  
   Q7_2 = Online courses as first-study resources also predicts salary, reinforcing the importance of formal MOOC engagement

9. **Work Activity – Analyzing and understanding data to influence decisions**  
   Q28_1 asks which activities make up your role at work. Those who spend their day analyzing data to drive business decisions earn more on average, highlighting the premium on analytical impact

10. **Helpful Resource – Kaggle competitions/notebooks**  
   Q7_5 = Kaggle as an early learning platform likewise predicts salary, showing the value of hands-on, project-based experience


**Takeaways:**

- **Geography & education** (country, degree level) are your strongest signals.  
- **Early learning choices** (video vs. courses vs. competitions) and preferred media channels (YouTube, Kaggle, blogs) are next.  
- **On-the-job tasks** (analysis vs. infrastructure) and core tools (SQL, RStudio, Colab) also carry weight.  
- This importance only tells us which Questions/Answers improve the model but not which direction these indicators move salary

**2) What are the most prevalent tools (software) and techniques (methods) being applied by Data Scientists today?**
"""

# === Code Cell 8: prevalence of top tools (software) and techniques (methods) ===

# Define column groups for tools and techniques
tool_groups = {
    'Programming Languages': [c for c in df.columns if c.startswith('Q12_')],
    'IDEs':                   [c for c in df.columns if c.startswith('Q13_')],
    'Hosted Notebooks':       [c for c in df.columns if c.startswith('Q14_')],
    'Viz Libraries':          [c for c in df.columns if c.startswith('Q15_')],
    'ML Frameworks':          [c for c in df.columns if c.startswith('Q17_')],
}

technique_groups = {
    'ML Algorithms':          [c for c in df.columns if c.startswith('Q18_')],
    'CV Methods':             [c for c in df.columns if c.startswith('Q19_')],
    'NLP Methods':            [c for c in df.columns if c.startswith('Q20_')],
}

# Function to print prevalence
def show_prevalence(group_name, cols):
    counts = df[cols].sum().sort_values(ascending=False)
    pct    = (df[cols].mean() * 100).sort_values(ascending=False)
    print(f"--- {group_name} ---")
    for col in counts.index:
        print(f"{col}: {counts[col]} respondents ({pct[col]:.1f}%)")
    print()

# Display most prevalent tools
for name, cols in tool_groups.items():
    show_prevalence(name, cols)

# Display most prevalent techniques
for name, cols in technique_groups.items():
    show_prevalence(name, cols)

"""# === **What are the most prevalent tools (software) and techniques (methods) being applied by Data Scientists today?** ===
#### Interpretation of Prevalence of Top Tools and Techniques

**1. Programming Languages (Q12_*)**   
- **Python (Q12_1)** leads at **77.7%** (18,653 respondents), underscoring its status as the dominant language in data science.  
- **SQL (Q12_3)** follows with **40.1%**, reflecting the centrality of database querying.  
- **R (Q12_2)** and **Java (Q12_6)** each appear in ~**19.0%** of responses, indicating continued use of statistical scripting and general-purpose development.  
- **JavaScript (Q12_7)** at **16.1%** and **C (Q12_4)** at **15.8%** round out the top five, while languages like Julia (Q12_8, 14.5%) and Bash (Q12_9, 7.0%) serve specialized roles.  

**2. Integrated Development Environments (Q13_*)**   
- **Jupyter Notebook (Q13_11)** is used by **57.0%**, cementing its role for interactive analysis.  
- **VS Code (Q13_4)** comes in at **41.6%**, highlighting its appeal for code editing.  
- **PyCharm (Q13_5, 25.4%)**, **JupyterLab (Q13_1, 20.4%)**, and **Visual Studio (Q13_3, 18.4%)** complete the top five environments.  

**3. Hosted Notebooks (Q14_*)**   
- **Colab (Q14_2)** leads at **37.2%**, thanks to free compute and seamless Drive integration.  
- **Kaggle Notebooks (Q14_1)** follow at **31.2%**, reflecting community engagement.  
- All other platforms fall below **5%**, showing clear user concentration.  

**4. Visualization Libraries (Q15_*)**   
- **Matplotlib (Q15_1)** tops at **58.4%**, with **Seaborn (Q15_2)** at **43.8%**, illustrating Python’s core plotting stack dominance.  
- **Plotly (Q15_3)** at **21.2%** and **ggplot2 (Q15_4)** at **17.3%** highlight popular grammar-of-graphics workflows.  

**5. ML Frameworks & Algorithms (Q17\_*, Q18\_*)**   
- **Scikit-learn (Q17_1)**: **47.5%**, remains the go‑to for classical ML.  
- **TensorFlow (Q17_2)** at **33.1%** and **Keras (Q17_3)** at **27.4%**, with **PyTorch (Q17_4)** close behind at **21.6%**.  
- **Regression (Q18_1)** and **Trees/Forests (Q18_2)** capture **47.2%** and **39.1%** respectively, while **Gradient Boosting (Q18_7)** appears in **25.0%** of workflows.  

**6. CV & NLP Methods (Q19\_*, Q20\_*)**
- **Image Classification (Q19_4)** leads CV methods at **15.3%**, with segmentation and detection each at ~**10%**.  
- **Word Embeddings (Q20_1)** and **Transformers (Q20_4)** both near **9%**, reflecting their status as standard NLP building blocks.  

---

*Key takeaway: Python and its notebook environments dominate, classical ML remains foundational, and specialized domains (CV/NLP) are growing but still secondary in practitioner workflows.*

**3) What tools and techniques are emerging in the field of Data Science?**
"""

# === Code Cell 8: identify emerging tools and techniques ===
import re

# 1. Find all dummy-indicator columns (tools/techniques)
dummy_cols = [col for col in df.columns if re.fullmatch(r'Q\d+_\d+', col)]

# 2. Compute usage prevalence (fraction of respondents who use each)
prevalence = df[dummy_cols].mean().sort_values()

# 3. Select the lowest non-zero prevalence items (emerging tools/techniques)
emerging = prevalence[prevalence > 0].head(10)

# 4. Display results
print("Emerging tools/techniques (lowest non-zero usage):")
for col, frac in emerging.items():
    print(f"{col}: {frac:.1%}")

"""# === **What tools and techniques are emerging in the field of Data Science?** ===
**Enterprise BI Tools**

MicroStrategy (Q36_13) and Sisense (Q36_10) each register only 0.1–0.2% usage among respondents. These platforms sit far behind the likes of Power BI or QuickSight, suggesting they remain niche or early-stage choices for analysts ​kaggle_survey_2022_answ….

**Managed ML Platforms**

C3.ai (Q37_9) appears in just 0.1% of responses to “Which managed ML products do you use?” That low figure indicates C3.ai’s enterprise-scale orchestration services have yet to gain traction with individual practitioners ​kaggle_survey_2022_answ….

**Model-Monitoring Frameworks**

In the model-observability space (Q4032), Guild.ai (Q40_5), Aporia (Q40_8), WhyLabs (Q40_11) and Arize (Q40_10) all fall between 0.1–0.2% usage. This cluster of barely-adopted tools highlights that production-grade monitoring and explainability are still emerging concerns for most data scientists ​kaggle_survey_2022_answ….

**Responsible/​Ethical AI Toolkits**

Aequitas (Q41_7) from the responsible-AI suite shows just 0.2% uptake. Fairness-and-bias-detection libraries remain on the periphery of standard workflows, pointing to an area ripe for broader tooling and education ​kaggle_survey_2022_answ….

**Specialized Hardware**

Custom silicon like RDUs (Q42_5) and AWS Trainium Chips (Q42_6) clock in at 0.1–0.2% under “Which specialized hardware do you use?” This tiny footprint reflects that, outside of GPUs/TPUs, bespoke accelerators are only just beginning to permeate practitioners’ workstations

##**Key Takeaways**
All ten of these items—high-end BI suites, managed-service platforms, monitoring frameworks, fairness-toolkits, and custom silicon—are used by fewer than 0.2% of respondents. They represent the frontier of commercial offerings and specialized infrastructure; adoption remains nascent, signaling where the next waves of tooling and training could focus.

**4) How and where should aspiring data scientist invest their time and energy to prepare for the current and future Data Science environment?**

### 📦 Five Focus Areas for Aspiring Data Scientists

1. **Master Core Tools & Languages**  
   Invest time in Python (pandas, NumPy, scikit‑learn) and SQL within notebook environments like Jupyter, Colab, and Kaggle.  

2. **Solidify Statistical & ML Foundations**  
   Build expertise in classical algorithms (regression, decision trees, ensemble methods) using scikit‑learn before diving into deep learning.  

3. **Leverage Interactive Learning Platforms**  
   Complete structured MOOCs on Coursera/edX and follow high‑quality video tutorials on YouTube to accelerate hands‑on skill development.  

4. **Engage with the Community**  
   Participate in Kaggle competitions, share notebooks, and consume industry media (blogs, podcasts, newsletters) to stay connected and learn best practices.  

5. **Explore Emerging Practices**  
   Familiarize yourself with MLOps workflows (MLflow, Weights & Biases), model monitoring, fairness & ethics toolkits (Fairlearn, Aequitas), and cloud platforms (AWS, GCP) to prepare for future demands.

**5) Is formal education important to success as a Data Scientist?**
A formal degree is a powerful accelerator as Holding a Master’s or higher is one of the top predictors of higher salary in data science. Although, real-world projects, continuous self-education, and demonstrable results are equally (if not more) important for a thriving data-science career.

**6) How does the return on formal education compare to other types of learning?**
"""

# === Code Cell 9: Compare “return” (salary) of formal education vs. other learning paths ===

import pandas as pd

# 1. Map Q29 salary buckets to numeric midpoints
salary_map = {
    '$0-999': 500,
    '1,000-1,999': 1500,
    '2,000-2,999': 2500,
    '3,000-3,999': 3500,
    '4,000-4,999': 4500,
    '5,000-7,499': 6250,
    '7,500-9,999': 8750,
    '10,000-14,999': 12500,
    '15,000-19,999': 17500,
    '20,000-24,999': 22500,
    '25,000-29,999': 27500,
    '30,000-39,999': 35000,
    '40,000-49,999': 45000,
    '50,000-59,999': 55000,
    '60,000-69,999': 65000,
    '70,000-79,999': 75000,
    '80,000-89,999': 85000,
    '90,000-99,999': 95000,
    '$100,000-124,999': 112500,
    '$125,000-149,999': 137500,
    '$150,000-199,999': 175000,
    '$200,000-249,999': 225000,
    '$250,000-299,999': 275000,
    '$300,000-499,999': 400000,
    '$500,000-999,999': 750000,
    '>$1,000,000': 1000000
}

df['salary_mid'] = df['Q29'].map(salary_map)

# 2. Filter to respondents with salary information
df_sal = df.dropna(subset=['salary_mid'])

# 3. Average salary by formal education level (Q8)
edu_return = df_sal.groupby('Q8')['salary_mid'].mean().sort_values(ascending=False)
print("Average salary by education level:\n", edu_return, "\n")

# 4. Compare average salary for key alternative learning sources
learning_paths = {
    'Coursera': 'Q6_1',
    'edX': 'Q6_2',
    'Kaggle Learn Courses': 'Q6_3',
    'Video platforms (YouTube, Twitch)': 'Q7_4'
}

print("Average salary by learning path usage:\n")
for label, col in learning_paths.items():
    used_mean = df_sal[df_sal[col] == 1]['salary_mid'].mean()
    not_used_mean = df_sal[df_sal[col] == 0]['salary_mid'].mean()
    print(f"{label}: used => ${used_mean:,.0f}, not used => ${not_used_mean:,.0f}")

"""#### **How does the return on formal education compare to other types of learning?**

The data reveal a clear, stepwise “return” on formal education: professional doctorate holders average about \$48 067, doctoral degree holders \$41 886, master’s \$34 130, some college without a bachelor’s \$33 149, bachelor’s \$29 579, and those with no formal education past high school \$28 167 (with non-disclosers at \$24 186). When comparing learning paths, structured MOOCs deliver a noticeable salary boost—Coursera users average \$36 822 versus \$31 443 for non-users, and edX users \$37 132 versus \$33 323—whereas more informal channels like Kaggle Learn (\$32 388 vs. \$34 383) and general video platforms (\$33 066 vs. \$34 650) align with slightly lower earnings, suggesting that accredited programs confer the greatest measurable premium, while ad-hoc or community-based learning is more common among those earlier in their careers.

**7) How should educational institutions think about the role of formal education in the world of data science? Are there any specific recommendations in terms of methodologies that institutions of formal education should employ in the training of data scientists?**

# === Code Cell 10: Text Box – Recommendations for Formal Education in Data
### 🎓 Role of Formal Education in Data Science: Institutional Recommendations

1. **Balance Theory with Hands‑On Practice**  
   Combine rigorous coverage of statistical and machine‑learning fundamentals (e.g. regression, decision trees, ensemble methods) with in‑class labs using Python & scikit‑learn to reinforce concepts.

2. **Adopt Interactive Notebook Workflows**  
   Integrate Jupyter‑style notebooks (JupyterLab, Colab, Kaggle) as primary teaching tools, allowing students to write, execute, and annotate code directly alongside narrative explanations.

3. **Leverage Blended & Micro‑Credential Models**  
   Partner with credible MOOC platforms (Coursera, edX) to offer modular, flipped‑classroom content—students complete guided online modules before in‑person sessions to maximize engagement and depth.

4. **Embed MLOps, Ethics & Emerging Tools Early**  
   Introduce core MLOps workflows (MLflow, Weights & Biases) and fairness/bias detection toolkits (Fairlearn, Aequitas) within project courses, ensuring familiarity with production best practices and responsible AI.

5. **Foster Industry Engagement & Community Learning**  
   Encourage participation in Kaggle competitions, hackathons, and open‑source contributions. Incorporate real‑world datasets and industry case studies to bridge academic learning with current workplace demands.
"""

# === Code Cell 10: Split data & Random Forest to predict salary_mid ===

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import pandas as pd

# 1. Ensure salary_mid exists (map if not already)
if 'salary_mid' not in df.columns:
    salary_map = {
        '$0-999': 500, '1,000-1,999': 1500, '2,000-2,999': 2500, '3,000-3,999': 3500,
        '4,000-4,999': 4500, '5,000-7,499': 6250, '7,500-9,999': 8750,
        '10,000-14,999': 12500, '15,000-19,999': 17500, '20,000-24,999': 22500,
        '25,000-29,999': 27500, '30,000-39,999': 35000, '40,000-49,999': 45000,
        '50,000-59,999': 55000, '60,000-69,999': 65000, '70,000-79,999': 75000,
        '80,000-89,999': 85000, '90,000-99,999': 95000, '$100,000-124,999': 112500,
        '$125,000-149,999': 137500, '$150,000-199,999': 175000, '$200,000-249,999': 225000,
        '$250,000-299,999': 275000, '$300,000-499,999': 400000,
        '$500,000-999,999': 750000, '>$1,000,000': 1000000
    }
    df['salary_mid'] = df['Q29'].map(salary_map)

# 2. Filter to rows with salary information
df_model = df.dropna(subset=['salary_mid']).copy()

# 3. Prepare features and target
X = df_model.drop(columns=['Q29', 'salary_mid'])
X_enc = pd.get_dummies(X, dummy_na=False)
y = df_model['salary_mid']

# 4. Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_enc, y, test_size=0.2, random_state=42
)

# 5. Train Random Forest
rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)

# 6. Identify top 10 most important predictors
importances = pd.Series(rf.feature_importances_, index=X_enc.columns)
top10 = importances.nlargest(10)

print("Top 10 features most predictive of salary_mid:")
print(top10)

"""#**This is our final model, you can use this code to create predictions, we also found out how to download the model as a pickle file, the only prolem was that we couldnt figure out how to uplaod our to Streamlit, we even have a repository on GitHub**"""

# === Code Cell 11: Interactive Salary Predictor Model (with ipywidgets install) ===

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import ipywidgets as widgets
from IPython.display import display

# 1. Map salary buckets to numeric midpoint
salary_map = {
    '$0-999': 500, '1,000-1,999': 1500, '2,000-2,999': 2500, '3,000-3,999': 3500,
    '4,000-4,999': 4500, '5,000-7,499': 6250, '7,500-9,999': 8750,
    '10,000-14,999': 12500, '15,000-19,999': 17500, '20,000-24,999': 22500,
    '25,000-29,999': 27500, '30,000-39,999': 35000, '40,000-49,999': 45000,
    '50,000-59,999': 55000, '60,000-69,999': 65000, '70,000-79,999': 75000,
    '80,000-89,999': 85000, '90,000-99,999': 95000, '$100,000-124,999': 112500,
    '$125,000-149,999': 137500, '$150,000-199,999': 175000,
    '$200,000-249,999': 225000, '$250,000-299,999': 275000,
    '$300,000-499,999': 400000, '$500,000-999,999': 750000,
    '>$1,000,000': 1000000
}
df['salary_mid'] = df['Q29'].map(salary_map)

# 2. Select and encode predictors
predictors = ['Q4','Q11','Q30','Q27','Q23','Q25'] + [f'Q34_{i}' for i in range(1,9)]
model_df = df.dropna(subset=['salary_mid'] + predictors).copy()
X = pd.get_dummies(model_df[predictors], dummy_na=False)
y = model_df['salary_mid']

# 3. Train/test split and model fitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

# 4. Evaluate on test set
y_pred = model.predict(X_test)
print("Test RMSE:", round(mean_squared_error(y_test, y_pred) ** 0.5, 0))  # Calculate RMSE manually
print("Test R²:", round(r2_score(y_test, y_pred), 3))

# 5. Define input widgets
country_opts = ['Algeria','Argentina','Australia','Bangladesh','Belgium','Brazil','Cameroon','Canada',
                'Chile','China','Colombia','Czech Republic','Ecuador','Egypt','Ethiopia','France',
                'Germany','Ghana','Hong Kong (S.A.R.)','I do not wish to disclose my location',
                'India','Indonesia','Iran, Islamic Republic of...','Ireland','Israel','Italy','Japan',
                'Kenya','Malaysia','Mexico','Morocco','Nepal','Netherlands','Nigeria','Other',
                'Pakistan','Peru','Philippines','Poland','Portugal','Romania','Russia','Saudi Arabia',
                'Singapore','South Africa','South Korea','Spain','Sri Lanka','Taiwan','Thailand',
                'Tunisia','Turkey','Ukraine','United Arab Emirates',
                'United Kingdom of Great Britain and Northern Ireland','United States of America',
                'Viet Nam','Zimbabwe']
coding_opts = ['< 1 years','1-3 years','3-5 years','5-10 years','10-20 years','20+ years','I have never written code']
spend_opts = ['$0 ($USD)','$1-$99','$100-$999','$1,000-$9,999','$10,000-$99,999','$100,000 or more ($USD)']
ml_maturity_opts = [
    'I do not know','No (we do not use ML methods)',
    'We are exploring ML methods (and may one day put a model into production)',
    'We recently started using ML methods (i.e., models in production for less than 2 years)',
    'We have well established ML methods (i.e., models in production for more than 2 years)',
    'We use ML methods for generating insights (but do not put working models into production)'
]
role_opts = [
    'Currently not employed','Data Administrator',
    'Data Analyst (Business, Marketing, Financial, Quantitative, etc)',
    'Data Architect','Data Engineer','Data Scientist','Developer Advocate',
    'Engineer (non-software)','Machine Learning/ MLops Engineer',
    'Manager (Program, Project, Operations, Executive-level, etc)',
    'Other','Research Scientist','Software Engineer','Statistician','Teacher / professor'
]
company_opts = ['0-49 employees','50-249 employees','250-999 employees','1000-9,999 employees','10,000 or more employees']
storage_opts = [
    'Amazon S3','Amazon Elastic File System (EFS)','Google Cloud Storage (GCS)',
    'Google Cloud Filestore','Microsoft Azure Blob Storage','Microsoft Azure Files',
    'No / None','Other'
]

w_country = widgets.Dropdown(options=country_opts, description='In which country do you currently reside?')
w_code    = widgets.Dropdown(options=coding_opts, description='For how many years have you been writing code and/or programming?')
w_spend   = widgets.Dropdown(options=spend_opts, description="Approximately how much money have you spent on machine learning and/or cloud computing services at home or at work in the past 5 years (approximate $USD)?")
w_maturity= widgets.Dropdown(options=ml_maturity_opts, description='Does your current employer incorporate machine learning methods into their business?')
w_role    = widgets.Dropdown(options=role_opts, description='Select the title most similar to your current role (or most recent title if retired):')
w_company = widgets.Dropdown(options=company_opts, description='What is the size of the company where you are employed?')
w_store   = widgets.Dropdown(options=storage_opts, description='Do you use any of the following data storage products? (Select all that apply)')

for w in [w_country, w_code, w_spend, w_maturity, w_role, w_company, w_store]:
    display(w)

# 6. Prediction function
def predict_salary(_):
    inp = {
        'Q4': w_country.value,
        'Q11': w_code.value,
        'Q30': w_spend.value,
        'Q27': w_maturity.value,
        'Q23': w_role.value,
        'Q25': w_company.value
    }
    # Q34 one-hot
    for i, opt in enumerate(storage_opts, start=1):
        inp[f'Q34_{i}'] = 1 if w_store.value == opt else 0
    df_input = pd.DataFrame([inp])
    df_enc = pd.get_dummies(df_input)
    df_enc = df_enc.reindex(columns=X_train.columns, fill_value=0)
    pred = model.predict(df_enc)[0]
    print(f"✅ Predicted annual midpoint salary: ${pred:,.0f}")

btn = widgets.Button(description="Predict Salary")
btn.on_click(predict_salary)
display(btn)

# === Code Cell 12: Mount Drive & Save Trained Model ===

import pickle

# Path under your Drive
filename = '/content/drive/MyDrive/Colab Notebooks/salary_model.pkl'

# `model` is the fitted RandomForestRegressor from Code Cell 11
with open(filename, 'wb') as f:
    pickle.dump(model, f)

print(f"✅ Trained model saved to: {filename}")

# === Code Cell 11: Streamlit Interactive Salary Predictor ===
import streamlit as st
import pandas as pd
import pickle

# Load the trained pipeline
pipeline = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/salary_model.pkl','rb'))

# --- 1. Define options ---
country_opts = [
    'Algeria','Argentina','Australia','Bangladesh','Belgium','Brazil','Cameroon','Canada',
    'Chile','China','Colombia','Czech Republic','Ecuador','Egypt','Ethiopia','France',
    'Germany','Ghana','Hong Kong (S.A.R.)','I do not wish to disclose my location',
    'India','Indonesia','Iran, Islamic Republic of...','Ireland','Israel','Italy','Japan',
    'Kenya','Malaysia','Mexico','Morocco','Nepal','Netherlands','Nigeria','Other',
    'Pakistan','Peru','Philippines','Poland','Portugal','Romania','Russia','Saudi Arabia',
    'Singapore','South Africa','South Korea','Spain','Sri Lanka','Taiwan','Thailand',
    'Tunisia','Turkey','Ukraine','United Arab Emirates',
    'United Kingdom of Great Britain and Northern Ireland','United States of America',
    'Viet Nam','Zimbabwe'
]
coding_opts = ['< 1 years','1-3 years','3-5 years','5-10 years','10-20 years','20+ years','I have never written code']
spend_opts = ['$0 ($USD)','$1-$99','$100-$999','$1,000-$9,999','$10,000-$99,999','$100,000 or more ($USD)']
ml_maturity_opts = [
    'I do not know','No (we do not use ML methods)',
    'We are exploring ML methods (and may one day put a model into production)',
    'We recently started using ML methods (i.e., models in production for less than 2 years)',
    'We have well established ML methods (i.e., models in production for more than 2 years)',
    'We use ML methods for generating insights (but do not put working models into production)'
]
role_opts = [
    'Currently not employed','Data Administrator',
    'Data Analyst (Business, Marketing, Financial, Quantitative, etc)',
    'Data Architect','Data Engineer','Data Scientist','Developer Advocate',
    'Engineer (non-software)','Machine Learning/ MLops Engineer',
    'Manager (Program, Project, Operations, Executive-level, etc)',
    'Other','Research Scientist','Software Engineer','Statistician','Teacher / professor'
]
company_opts = ['0-49 employees','50-249 employees','250-999 employees','1000-9,999 employees','10,000 or more employees']
storage_opts = [
    'Amazon S3','Amazon Elastic File System (EFS)','Google Cloud Storage (GCS)',
    'Google Cloud Filestore','Microsoft Azure Blob Storage','Microsoft Azure Files',
    'No / None','Other'
]

# --- 2. Collect user input ---
st.title("🔮 Data Science Salary Predictor")
country    = st.selectbox("Country of Residence", country_opts)
coding     = st.selectbox("Years Writing Code", coding_opts)
spend      = st.selectbox("5-yr ML/Cloud Spend", spend_opts)
maturity   = st.selectbox("ML in Your Organization", ml_maturity_opts)
role       = st.selectbox("Your Current/Most Recent Role", role_opts)
company    = st.selectbox("Company Size", company_opts)
storage    = st.multiselect("Data Storage Products You Use", storage_opts)

# --- 3. Predict on button click ---
if st.button("Predict Salary"):
    # build feature dict
    inp = {
        'Q4': country,
        'Q11': coding,
        'Q30': spend,
        'Q27': maturity,
        'Q23': role,
        'Q25': company
    }
    # one-hot encode storage usage
    for i, opt in enumerate(storage_opts, start=1):
        inp[f'Q34_{i}'] = 1 if opt in storage else 0

    df_in = pd.DataFrame([inp])
    pred = pipeline.predict(df_in)[0]
    st.subheader(f"Estimated Annual Midpoint Salary:\nUS$ {pred:,.0f}")